# Essay Content and Household Income: Is it a True Holistic Approach? 

## Overview 
Past research has expressed a strong correlation between SAT scoring and household income, but very few studies have examined the connection between household income and essay structure. Based on the article, ‘Essay content and style are strongly related to household income and SAT scores: Evidence from 60,000 undergraduate applications,’ we plan to reassess the original methods used and test the relationship between essay features (topics and dictionaries) to household income. 
In the original essay, researchers used both Correlated Topic Modeling (CTM) and Linguistic Inquiry and Word Count (LIWC) to decipher the connection between income, SAT scores, essay content, and essay styling. However, in order to simplify the methods, we wanted to conduct an additional analysis through applying BERT embeddings to the dataset then running regressions on those embeddings. Therefore, our research question is: Could adding BERT embeddings and running additional regression models, lead to better R-squared results compared to the original essays? With this research question, we plan on discovering the limitations and strengths in our original papers methods. To do so, we employed the original articles' preprocessed dataset containing the topics and features of 60,000 UC college essays for our BERT analysis. 

## Data: 
We used the data given to us by the researchers in the article. As mentioned above, the dataset is made of college application essays from roughly 60,000 applicants from the 2016 application year. Each applicant wrote four essays which were merged into one essay. In order to achieve this dataset the researchers decided to break these essays up into content of the essay and style of the essay. The content of the essay can be found through essay topics and the style of the essay through dictionary features. The researchers used Correlated Topic Modeling (CTM) to group together similar topics in the essays and ended up generating 70 topics, which we have labeled topic_names in our code. The researchers used Linguistic Inquiry and Word Count(LIWC) to extract similar words and characters in the essays and included those alongside an external dictionary to create categories that ‘model pattern of writing style’ (Alvero et al., 2021). Thus, the dataset we used consisted of 70 columns correlated to specific topics in the essays, and 90 columns correlated to different dictionary features. There were exactly 60,000 observations in the dataset and within each of the rows was the proportion of how often a certain topic or dictionary feature appeared in the individual’s essay. 

## Methods I: Retrieving Summed BERT Embeddings
We knew that we wanted to implement word embeddings in some way and went through many different techniques to see how we could incorporate them into our research question. We had originally wanted to use word embeddings to see if we could extract the top three topics for high income and low income applicants. However, this proved to not really give us any new information and so we instead decided to run BERT on both the topics and dictionary features to extract the summed embeddings for each applicant. To do this, we manually set which columns in the dataset correspond to topics and which columns correspond to dictionary features. Then, we created a loop to iterate through each of the rows, while creating a numpy array to add the summed embeddings to. To retrieve the actual embeddings, we calculated the weight of each topic in the row, the embedding of each topic, and added them together and placed them back into the numpy array we created earlier. Finally, we normalized the summed embeddings by dividing it by the length of how many topic columns there originally were. We then followed the exact same steps to calculate the summed embeddings for the dictionary features.

## Method II: Regression Analysis 
After we added the BERT embeddings to the dataset, we ran a regression analysis to predict family income using the original columns in the dataset and the summed embeddings as the independent variables. We ran two separate regressions: one for dictionary features and another for topics. Running these regressions was important because it would allow us to compare R-squared values with the original paper to decipher which is the better predictor: the higher the R-squares, the better the model. 

In order to perform our regression, we utilized the initial code our researchers used, but added the summed BERT embeddings. This allowed us to not only look at dictionary/ topic features as independent variables, but also summed embeddings. In addition to this, utilizing the initial code allowed us to calculate the R-squared through a 10 fold cross validation. The purpose of using a 10 fold cross validation was to ensure there would be no overfitting and validate a reliable regression (Alvero 2021). Moreover, within our regression we also had the RMSE predict our mean squared errors. The use of mean squared error is important because it allowed us to identify how well our regression model was running. The larger the MSE, the less confident our methods are. Finally, we ran an ordinary least squares regression that presented us with an adjusted R-squared. 

## Findings: 
After running the regression on both summed embeddings we found the following:  Dictionary features + summed dictionary embeddings produced an adjusted R-squared of 13% compared to original authors 0.129. The average mean squared average resulted in 16471341235.27767 which indicates there is unreliable accuracy within our methods. Topic features + summed embeddings produced an adjusted R-squared of 17% compared to original authors R2  = 16%.  The average mean squared average resulted in 15505853603.688473 which implies there is an inaccuracy in our methods. 
In addition to our regression analysis, in order to further understand and explain our results we added an income docile graph inspired from our original paper's figures. In this figure, income is organized across ten groups, marking ten as the highest form of income. Our findings show that in both dictionaries and topics, R squared tends to be highest in the 10th decile. This means both dictionaries (R2= 4-6%) and topics (R2=10%) have a stronger relationship to high income households. Through these findings, we can conclude our model did trivial change to the original methods R-Squared. However, we did see a one percent increase in our R-squared for topic embeddings. 

